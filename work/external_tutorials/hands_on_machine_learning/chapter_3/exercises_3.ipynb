{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "**Q3**: Tackle the Titanic dataset. A great place to start is on Kaggle.\n",
    "\n",
    "**A3**: Kaggle solution developed [here](../../../kaggle/titanic/index.ipynb).\n",
    "\n",
    "**Q4**: Build a spam classifier (a more challenging exercise):\n",
    "\n",
    "Download examples of spam and ham from [Apache SpamAssassin’s public datasets](http://spamassassin.apache.org/old/publiccorpus/).\n",
    "\n",
    "Unzip the datasets and familiarize yourself with the data format.\n",
    "\n",
    "Split the datasets into a training set and a test set.\n",
    "\n",
    "Write a data preparation pipeline to convert each email into a feature vector. Your preparation pipeline should transform an email into a (sparse) vector indicating the presence or absence of each possible word. For example, if all emails only ever contain four words, “Hello,” “how,” “are,” “you,” then the email “Hello you Hello Hello you” would be converted into a vector [1, 0, 0, 1] (meaning [“Hello” is present, “how” is absent, “are” is absent, “you” is present]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of each word.\n",
    "\n",
    "You may want to add hyperparameters to your preparation pipeline to control whether or not to strip off email headers, convert each email to lowercase, remove punctuation, replace all URLs with “URL,” replace all numbers with “NUMBER,” or even perform stemming (i.e., trim off word endings; there are Python libraries available to do this).\n",
    "\n",
    "Then try out several classifiers and see if you can build a great spam classifier, with both high recall and high precision.\n",
    "\n",
    "\n",
    "**A4**: Solution below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the ReadME:\n",
    "\n",
    "OK, now onto the corpus description.  It's split into three parts, as follows:\n",
    "\n",
    "  - spam: 500 spam messages, all received from non-spam-trap sources.\n",
    "\n",
    "  - easy_ham: 2500 non-spam messages.  These are typically quite easy to\n",
    "    differentiate from spam, since they frequently do not contain any spammish\n",
    "    signatures (like HTML etc).\n",
    "\n",
    "  - hard_ham: 250 non-spam messages which are closer in many respects to\n",
    "    typical spam: use of HTML, unusual HTML markup, coloured text,\n",
    "    \"spammish-sounding\" phrases etc.\n",
    "\n",
    "  - easy_ham_2: 1400 non-spam messages.  A more recent addition to the set.\n",
    "\n",
    "  - spam_2: 1397 spam messages.  Again, more recent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's download each of:\n",
    "# - spam\n",
    "# - spam_2\n",
    "# - easy_ham\n",
    "# - easy_ham_2\n",
    "# - hard_ham\n",
    "\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "LOCAL_DATA_DIR = './tmp/'\n",
    "\n",
    "file_names = [\n",
    "    '20030228_spam.tar.bz2',\n",
    "    '20050311_spam_2.tar.bz2',\n",
    "    '20030228_easy_ham.tar.bz2',\n",
    "    '20030228_easy_ham_2.tar.bz2',\n",
    "    '20030228_hard_ham.tar.bz2',\n",
    "]\n",
    "\n",
    "dirs = [\n",
    "    'spam',\n",
    "    'spam_2',\n",
    "    'easy_ham',\n",
    "    'easy_ham_2',\n",
    "    'hard_ham',    \n",
    "]\n",
    "\n",
    "def fetch_file(file_name):\n",
    "    download_path = LOCAL_DATA_DIR + file_name\n",
    "    file_url = DOWNLOAD_ROOT + file_name\n",
    "    if not (os.path.exists(download_path)):\n",
    "        os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
    "        tgz_path = os.path.join(LOCAL_DATA_DIR, file_name)\n",
    "        urllib.request.urlretrieve(file_url, tgz_path)\n",
    "        spam_tgz = tarfile.open(tgz_path)\n",
    "        spam_tgz.extractall(path=LOCAL_DATA_DIR)\n",
    "        spam_tgz.close()\n",
    "\n",
    "for file_name in file_names:\n",
    "    fetch_file(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(LOCAL_DATA_DIR +'/easy_ham_2/' + '00001.1a31cc283af0060967a233d26548a6ce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "message = email.message_from_file(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c630824e7e67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "payload = message.get_payload(decode=False)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(payload, 'html5lib')\n",
    "\n",
    "raw_text = soup.get_text(strip=True)\n",
    "import nltk\n",
    "\n",
    "tokens = nltk.word_tokenize(raw_text)\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "[porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = set(text6)\n",
    "long_words = [w for w in V if len(w) > 10]\n",
    "sorted(long_words)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text6.collocations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
